# -*- coding: utf-8 -*-
"""Final_Project_AML

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CuEwDXLAb6ak-em8iG33iKaKRH_uRuOa
"""

#IMPORTS
import numpy as np
import pandas as pd
import sklearn.model_selection as modelselection
import sklearn.metrics as metrics
import seaborn as sns
from collections import Counter
from sklearn.datasets import make_classification
from imblearn.over_sampling import SMOTE   
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.impute import SimpleImputer
from matplotlib import pyplot as plt 
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier

#AutoEncoder - Elham

def Autoencoder_Model():                                      #https://www.nature.com/articles/s41598-021-93543-8#Sec8
    from keras.models import Sequential
    from keras.layers import Dense
    model=Sequential()
    model.add(Dense(58,activation='relu',input_shape=(58,)))
    model.add(Dense(32,activation='relu'))
    model.add(Dense(58,activation='relu'))
    model.compile(optimizer='adam',loss='binary_crossentropy')
    return model

# create new data by autoencoder
def create_300_data_approx():
    from sklearn.model_selection import KFold
    new_data=np.zeros((1,58))
    kfold=KFold(n_splits=3,random_state=1,shuffle=True)
    for train,test in kfold.split(initial_data):
        x_train=initial_data[train]
        x_test=initial_data[test]
        model=Autoencoder_Model()
        model.fit(x_train,x_train,batch_size=512, epochs=50, validation_split=0.1, verbose = 0)
        predicts=model.predict(x_test)
        new_data=np.concatenate((new_data,predicts))
        new_data=new_data[1:]
    return new_data


# read data
path=r'/content/extention of Z-Alizadeh sani dataset.xlsx'
dataset=pd.read_excel(path)
df=pd.DataFrame(dataset)
df = df.drop('Exertional CP', axis = 1)
df=df.replace({'Male':1,'Fmale':0,'Y':1,'N':0,'Normal':0,'Stenotic':1
,'CAD':1,'mild':1,'Severe':2,'Moderate':3,'LBBB':1,'RBBB':2})
df_scaler = df.drop(['Function Class', 'BBB', 'Region RWMA', 'VHD'], axis = 1)
df_without_scaler = df[['Function Class', 'BBB', 'Region RWMA', 'VHD']]
initial_keys_scaler = df_scaler.keys()
initial_keys_without_scaler = df_without_scaler.keys()
initial_keys = np.concatenate((initial_keys_scaler,initial_keys_without_scaler))
initial_data_scaler = np.array(df_scaler)
initial_data_without_scaler = np.array(df_without_scaler)

# preprocessing
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    #Advanced ML Lecture ECE 5424
initial_data_scaler = imputer.fit_transform(initial_data_scaler)
initial_data_without_scaler = imputer.fit_transform(initial_data_without_scaler)
from sklearn.preprocessing import MinMaxScaler
scaler1=MinMaxScaler(feature_range = (-1,1))
initial_data_scaler = scaler1.fit_transform(initial_data_scaler)
initial_data = np.concatenate((initial_data_scaler, initial_data_without_scaler), axis = 1)
augmented_data=initial_data.copy()
print('Initial Data Shape: ',initial_data.shape)

for i in range(24):
    new_data=create_300_data_approx()
    augmented_data=np.concatenate((augmented_data,new_data))

# save final data
df_augmented = pd.DataFrame(augmented_data,columns=initial_keys)
final_data_scaler = df_augmented.drop(['Function Class', 'BBB', 'Region RWMA', 'VHD'], axis = 1)
final_data_without_scaler = df_augmented[['Function Class', 'BBB', 'Region RWMA', 'VHD']]

#preprocessing
final_data_scaler =scaler1.inverse_transform(final_data_scaler)
final_data_scaler = pd.DataFrame(final_data_scaler, columns = initial_keys_scaler)
df_augmented = pd.concat([final_data_scaler, final_data_without_scaler], axis = 1)
df_augmented.to_csv('Augmented_Data_7500.csv')

#SMOTE - Sakshi
path=r'/content/extention of Z-Alizadeh sani dataset.xlsx'     #https://machinelearningmastery.com/smote-oversampling-for-imbalanced-classification/
dataset=pd.read_excel(path)
df=pd.DataFrame(dataset)
df = df.drop('Exertional CP', axis = 1)
df = df.replace({'Male':1,'Fmale':0,'Y':1,'N':0,'Normal':0,'Stenotic':1
,'CAD':1,'mild':1,'Severe':2,'Moderate':3,'LBBB':1,'RBBB':2})
df_scaler = df.drop(['Function Class', 'BBB', 'Region RWMA', 'VHD'], axis = 1)
df_without_scaler = df[['Function Class', 'BBB', 'Region RWMA', 'VHD']]
initial_keys_scaler = df_scaler.keys()
initial_keys_without_scaler = df_without_scaler.keys()
initial_keys = np.concatenate((initial_keys_scaler,initial_keys_without_scaler))
initial_data_scaler = np.array(df_scaler)
initial_data_without_scaler = np.array(df_without_scaler)


# preprocessing
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    #Advanced ML Lecture ECE 5424
initial_data_scaler = imputer.fit_transform(initial_data_scaler)
initial_data_without_scaler = imputer.fit_transform(initial_data_without_scaler)
from sklearn.preprocessing import MinMaxScaler
scaler1=MinMaxScaler(feature_range = (-1,1))
initial_data_scaler = scaler1.fit_transform(initial_data_scaler)
initial_data = np.concatenate((initial_data_scaler, initial_data_without_scaler), axis = 1)
# Splitting into target variable
df_initial = pd.DataFrame(initial_data,columns=initial_keys)
X = df_initial
y = df_initial['Cath']
oversample = SMOTE(sampling_strategy = 1)
X, y = oversample.fit_resample(X, y)
df_augmented = pd.DataFrame(X)
final_data_scaler = df_augmented.drop(['Function Class', 'BBB', 'Region RWMA', 'VHD'], axis = 1)
final_data_without_scaler = df_augmented[['Function Class', 'BBB', 'Region RWMA', 'VHD']]

#preprocessing
final_data_scaler =scaler1.inverse_transform(final_data_scaler)
final_data_scaler = pd.DataFrame(final_data_scaler, columns = initial_keys_scaler)
df_augmented = pd.concat([final_data_scaler, final_data_without_scaler], axis = 1)
df_augmented.to_csv('Augmented_Data_SMOTE.csv')

#Thresholding - Vivek , Sakshi

df = pd.read_csv('/content/Augmented_Data_7500.csv')

def closest_value(input_list, input_value):   #https://www.entechin.com/find-nearest-value-list-python/
  arr = np.asarray(input_list)
  i = (np.abs(arr - input_value)).argmin()
  return i

def threshold(column_name):

  val = len(df[df[column_name] == 1])/len(df[df[column_name] == 0])
  list_1 = np.arange(0.1,1,0.0001)
  val_comparison_list = []
  for i in list_1:
    val_comparison_list.append(len(df[df[column_name] >=i])/len(df[df[column_name] < i]))
  index = closest_value(val_comparison_list, val)
  threshold_val = list_1[index]
  return threshold_val

binary_features = ['Sex','DM','HTN','Current Smoker','EX-Smoker','FH','Obesity','CRF','CVA','Airway disease',
        'Thyroid Disease', 'CHF', 'DLP','Edema',
       'Weak Peripheral Pulse', 'Lung rales', 'Systolic Murmur',
       'Diastolic Murmur', 'Typical Chest Pain', 'Dyspnea','Atypical',
       'Nonanginal', 'LowTH Ang','Q Wave', 'St Elevation', 'St Depression',
       'Tinversion', 'LVH', 'Poor R Progression','LAD', 'LCX', 'RCA', 'Cath']
continuous_features = ['Age','Weight','Length','BMI','BP', 'PR','FBS','CR', 'TG', 'LDL',
       'HDL', 'BUN', 'ESR', 'HB', 'K', 'Na', 'WBC', 'Lymph', 'Neut', 'PLT','EF-TTE']
categorical_features = ['Function Class', 'BBB','Region RWMA', 'VHD'] 


#Converting Data
for column in binary_features:
  th = threshold(column)
  df[column]=[1 if x>th else 0 for x in df[column]]

for column in categorical_features:
  df[column]=[round(x) for x in df[column]]

df.to_csv('Augmented_Data_7500_Threshold.csv')

#One hot encoding - Vivek
df = pd.read_csv('/content/Augmented_Data_7500_Threshold.csv')  

ohe = OneHotEncoder(sparse = False)                             #https://www.javatpoint.com/how-to-one-hot-encode-sequence-data-in-python
df_cat = pd.DataFrame()
cat_feat=['Function Class', 'BBB', 'Region RWMA', 'VHD']

for i in range(len(cat_feat)):
    encoded_data = ohe.fit_transform(df[[cat_feat[i]]])  
    #transform numpy array to a dataframe
    arr=ohe.categories_[0]
    newarr=[]
    for x in range(len(arr)):
      place = cat_feat[i] +'_'+ str(arr[x])
      newarr.append(place)
    df_encoded = pd.DataFrame(encoded_data, index=df.index, columns = newarr)
    df_cat = pd.concat([df_cat, df_encoded], axis = 1)

#concatenate all the dataframes
df = pd.concat([df, df_cat], axis=1)
cat_feat=['Function Class', 'BBB', 'Region RWMA', 'VHD']
df = df.drop(cat_feat,axis=1)

df.to_csv('Augmented_Data_7500_FINAL.csv')

#Logistic Regression - Sakshi

#LAD
df = pd.read_csv('/content/Augmented_Data_7500_FINAL.csv')
X1 = df.drop(['LAD','LCX','RCA','Cath'], axis = 1)
y1 = df[['LAD']]
X1_train, X1_test, y1_train, y1_test = modelselection.train_test_split(X1, y1, test_size = 0.3, random_state = 42)
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    
imputer.fit_transform(X1_train)
X1_test = imputer.transform(X1_test)
scaler = MinMaxScaler(feature_range=(-1,1))
X1_train = scaler.fit_transform(X1_train)
X1_test = scaler.transform(X1_test)
y1_train = scaler.fit_transform(y1_train)
y1_test = scaler.transform(y1_test)
y1_train = y1_train.ravel()
y1_test = y1_test.ravel()


clf = LogisticRegression(max_iter = 10000, tol = 1e-6, C= 50)
clf.fit(X1_train, y1_train)
y1_pred = clf.predict(X1_test)



print(f'Accuracy is: {metrics.accuracy_score(y1_test,y1_pred)}\nF1 score is: {metrics.f1_score(y1_test, y1_pred)}')
sns.set(font_scale=2)
ax = sns.heatmap(metrics.confusion_matrix(y1_test,y1_pred), annot=True)  
plt.show()


#LCX
df = pd.read_csv('/content/Augmented_Data_7500_FINAL.csv')
X1 = df.drop(['LAD','LCX','RCA','Cath'], axis = 1)
y1 = df[['LCX']]
X1_train, X1_test, y1_train, y1_test = modelselection.train_test_split(X1, y1, test_size = 0.3, random_state = 42)
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    
imputer.fit_transform(X1_train)
X1_test = imputer.transform(X1_test)
scaler = MinMaxScaler(feature_range=(-1,1))
X1_train = scaler.fit_transform(X1_train)
X1_test = scaler.transform(X1_test)
y1_train = scaler.fit_transform(y1_train)
y1_test = scaler.transform(y1_test)
y1_train = y1_train.ravel()
y1_test = y1_test.ravel()
clf = LogisticRegression(max_iter = 10000, tol = 1e-6, C= 20)
clf.fit(X1_train, y1_train)
y1_pred = clf.predict(X1_test)

print(f'Accuracy is: {metrics.accuracy_score(y1_test,y1_pred)}\nF1 score is: {metrics.f1_score(y1_test, y1_pred)}')
sns.set(font_scale=2)
ax = sns.heatmap(metrics.confusion_matrix(y1_test,y1_pred), annot=True)  
plt.show()


#RCA
df = pd.read_csv('/content/Augmented_Data_7500_FINAL.csv')
X1 = df.drop(['LAD','LCX','RCA','Cath'], axis = 1)
y1 = df[['RCA']]
X1_train, X1_test, y1_train, y1_test = modelselection.train_test_split(X1, y1, test_size = 0.3, random_state = 42)
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    
imputer.fit_transform(X1_train)
X1_test = imputer.transform(X1_test)
scaler = MinMaxScaler(feature_range=(-1,1))
X1_train = scaler.fit_transform(X1_train)
X1_test = scaler.transform(X1_test)
y1_train = scaler.fit_transform(y1_train)
y1_test = scaler.transform(y1_test)
y1_train = y1_train.ravel()
y1_test = y1_test.ravel()
clf = LogisticRegression(max_iter = 10000, tol = 1e-6, C = 30)
clf.fit(X1_train, y1_train)
y1_pred = clf.predict(X1_test)

print(f'Accuracy is: {metrics.accuracy_score(y1_test,y1_pred)}\nF1 score is: {metrics.f1_score(y1_test, y1_pred)}')
sns.set(font_scale=2)
ax = sns.heatmap(metrics.confusion_matrix(y1_test,y1_pred), annot=True)  
plt.show()

#Neural Networks - Vivek

#LAD
df = pd.read_csv('/content/Augmented_Data_7500_FINAL.csv')
X1 = df.drop(['LAD','LCX','RCA','Cath'], axis = 1)
y1 = df[['LAD']]
X1_train, X1_test, y1_train, y1_test = modelselection.train_test_split(X1, y1, test_size = 0.3, random_state = 42)
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    
imputer.fit_transform(X1_train)
X1_test = imputer.transform(X1_test)
scaler = MinMaxScaler(feature_range=(-1,1))
X1_train = scaler.fit_transform(X1_train)
X1_test = scaler.transform(X1_test)
y1_train = scaler.fit_transform(y1_train)
y1_test = scaler.transform(y1_test)
y1_train = y1_train.ravel()
y1_test = y1_test.ravel()

clf = MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(200,200), random_state=42)
clf.fit(X1_train, y1_train)
y1_pred = clf.predict(X1_test)


print(f'Accuracy is: {metrics.accuracy_score(y1_test,y1_pred)}\nF1 score is: {metrics.f1_score(y1_test, y1_pred)}')
sns.set(font_scale=2)
ax = sns.heatmap(metrics.confusion_matrix(y1_test,y1_pred), annot=True)  
plt.show()

#LCX
df = pd.read_csv('/content/Augmented_Data_7500_FINAL.csv')
X1 = df.drop(['LAD','LCX','RCA','Cath'], axis = 1)
y1 = df[['LCX']]
X1_train, X1_test, y1_train, y1_test = modelselection.train_test_split(X1, y1, test_size = 0.3, random_state = 42)
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    
imputer.fit_transform(X1_train)
X1_test = imputer.transform(X1_test)
scaler = MinMaxScaler(feature_range=(-1,1))
X1_train = scaler.fit_transform(X1_train)
X1_test = scaler.transform(X1_test)
y1_train = scaler.fit_transform(y1_train)
y1_test = scaler.transform(y1_test)
y1_train = y1_train.ravel()
y1_test = y1_test.ravel()

clf = MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(300), random_state=42)
clf.fit(X1_train, y1_train)
y1_pred = clf.predict(X1_test)


print(f'Accuracy is: {metrics.accuracy_score(y1_test,y1_pred)}\nF1 score is: {metrics.f1_score(y1_test, y1_pred)}')
sns.set(font_scale=2)
ax = sns.heatmap(metrics.confusion_matrix(y1_test,y1_pred), annot=True)  
plt.show()

#RCA
df = pd.read_csv('/content/Augmented_Data_7500_FINAL.csv')
X1 = df.drop(['LAD','LCX','RCA','Cath'], axis = 1)
y1 = df[['RCA']]
X1_train, X1_test, y1_train, y1_test = modelselection.train_test_split(X1, y1, test_size = 0.3, random_state = 42)
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    
imputer.fit_transform(X1_train)
X1_test = imputer.transform(X1_test)
scaler = MinMaxScaler(feature_range=(-1,1))
X1_train = scaler.fit_transform(X1_train)
X1_test = scaler.transform(X1_test)
y1_train = scaler.fit_transform(y1_train)
y1_test = scaler.transform(y1_test)
y1_train = y1_train.ravel()
y1_test = y1_test.ravel()

clf = MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(300,300), random_state=42)
clf.fit(X1_train, y1_train)
y1_pred = clf.predict(X1_test)


print(f'Accuracy is: {metrics.accuracy_score(y1_test,y1_pred)}\nF1 score is: {metrics.f1_score(y1_test, y1_pred)}')
sns.set(font_scale=2)
ax = sns.heatmap(metrics.confusion_matrix(y1_test,y1_pred), annot=True)  
plt.show()

#Random Forest - Elham

#LAD
path = r'‪/content/7500_FINAL.csv'
df = pd.read_csv(path.strip("‪u202a"))
X1 = df.drop(['LAD','LCX','RCA','Cath'], axis = 1)
y1 = df[['LAD']]
X1_train, X1_test, y1_train, y1_test = modelselection.train_test_split(X1, y1, test_size = 0.3, random_state = 42)
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    
imputer.fit_transform(X1_train)
X1_test = imputer.transform(X1_test)
scaler = MinMaxScaler(feature_range=(-1,1))
X1_train = scaler.fit_transform(X1_train)
X1_test = scaler.transform(X1_test)
y1_train = scaler.fit_transform(y1_train)
y1_test = scaler.transform(y1_test)
y1_train = y1_train.ravel()
y1_test = y1_test.ravel()


clf = RandomForestClassifier(max_depth=10, random_state=42)
clf.fit(X1_train, y1_train)
y1_pred = clf.predict(X1_test)



print(f'Accuracy is: {metrics.accuracy_score(y1_test,y1_pred)}\nF1 score is: {metrics.f1_score(y1_test, y1_pred)}')
sns.set(font_scale=2)
ax = sns.heatmap(metrics.confusion_matrix(y1_test,y1_pred), annot=True)  
plt.show()


#LCX
path = r'‪/content/7500_FINAL.csv'
df = pd.read_csv(path.strip("‪u202a"))
X1 = df.drop(['LAD','LCX','RCA','Cath'], axis = 1)
y1 = df[['LCX']]
X1_train, X1_test, y1_train, y1_test = modelselection.train_test_split(X1, y1, test_size = 0.3, random_state = 42)
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    
imputer.fit_transform(X1_train)
X1_test = imputer.transform(X1_test)
scaler = MinMaxScaler(feature_range=(-1,1))
X1_train = scaler.fit_transform(X1_train)
X1_test = scaler.transform(X1_test)
y1_train = scaler.fit_transform(y1_train)
y1_test = scaler.transform(y1_test)
y1_train = y1_train.ravel()
y1_test = y1_test.ravel()

clf = RandomForestClassifier(max_depth=10, random_state=42)
clf.fit(X1_train, y1_train)
y1_pred = clf.predict(X1_test)

print(f'Accuracy is: {metrics.accuracy_score(y1_test,y1_pred)}\nF1 score is: {metrics.f1_score(y1_test, y1_pred)}')
sns.set(font_scale=2)
ax = sns.heatmap(metrics.confusion_matrix(y1_test,y1_pred), annot=True)  
plt.show()

#RCA
path = r'‪/content/7500_FINAL.csv'
df = pd.read_csv(path.strip("‪u202a"))
X1 = df.drop(['LAD','LCX','RCA','Cath'], axis = 1)
y1 = df[['RCA']]
X1_train, X1_test, y1_train, y1_test = modelselection.train_test_split(X1, y1, test_size = 0.3, random_state = 42)
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    
imputer.fit_transform(X1_train)
X1_test = imputer.transform(X1_test)
scaler = MinMaxScaler(feature_range=(-1,1))
X1_train = scaler.fit_transform(X1_train)
X1_test = scaler.transform(X1_test)
y1_train = scaler.fit_transform(y1_train)
y1_test = scaler.transform(y1_test)
y1_train = y1_train.ravel()
y1_test = y1_test.ravel()

clf = RandomForestClassifier(max_depth=10, random_state=42)
clf.fit(X1_train, y1_train)
y1_pred = clf.predict(X1_test)

print(f'Accuracy is: {metrics.accuracy_score(y1_test,y1_pred)}\nF1 score is: {metrics.f1_score(y1_test, y1_pred)}')
sns.set(font_scale=2)
ax = sns.heatmap(metrics.confusion_matrix(y1_test,y1_pred), annot=True)  
plt.show()

#XGBoost - Elham

#LAD
path = r'‪/content/7500_FINAL.csv'
#df = pd.read_csv(path.strip("‪u202a"))
X1 = df.drop(['LAD','LCX','RCA','Cath'], axis = 1)
y1 = df[['LAD']]
X1_train, X1_test, y1_train, y1_test = modelselection.train_test_split(X1, y1, test_size = 0.3, random_state = 42)
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    
imputer.fit_transform(X1_train)
X1_test = imputer.transform(X1_test)
scaler = MinMaxScaler(feature_range=(-1,1))
X1_train = scaler.fit_transform(X1_train)
X1_test = scaler.transform(X1_test)
y1_train = scaler.fit_transform(y1_train)
y1_test = scaler.transform(y1_test)
y1_train = y1_train.ravel()
y1_test = y1_test.ravel()


clf = GradientBoostingClassifier(learning_rate=1.0, max_depth=10, random_state=42)
clf.fit(X1_train, y1_train)
y1_pred = clf.predict(X1_test)



print(f'Accuracy is: {metrics.accuracy_score(y1_test,y1_pred)}\nF1 score is: {metrics.f1_score(y1_test, y1_pred)}')
sns.set(font_scale=2)
ax = sns.heatmap(metrics.confusion_matrix(y1_test,y1_pred), annot=True)  
plt.show()


#LCX
path = r'‪/content/7500_FINAL.csv'
#df = pd.read_csv(path.strip("‪u202a"))
X1 = df.drop(['LAD','LCX','RCA','Cath'], axis = 1)
y1 = df[['LCX']]
X1_train, X1_test, y1_train, y1_test = modelselection.train_test_split(X1, y1, test_size = 0.3, random_state = 42)
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    
imputer.fit_transform(X1_train)
X1_test = imputer.transform(X1_test)
scaler = MinMaxScaler(feature_range=(-1,1))
X1_train = scaler.fit_transform(X1_train)
X1_test = scaler.transform(X1_test)
y1_train = scaler.fit_transform(y1_train)
y1_test = scaler.transform(y1_test)
y1_train = y1_train.ravel()
y1_test = y1_test.ravel()

clf = GradientBoostingClassifier(learning_rate=1.0, max_depth=10, random_state=42)
clf.fit(X1_train, y1_train)
y1_pred = clf.predict(X1_test)

print(f'Accuracy is: {metrics.accuracy_score(y1_test,y1_pred)}\nF1 score is: {metrics.f1_score(y1_test, y1_pred)}')
sns.set(font_scale=2)
ax = sns.heatmap(metrics.confusion_matrix(y1_test,y1_pred), annot=True)  
plt.show()

#RCA
path = r'/content/7500_FINAL.csv'
#df = pd.read_csv(path.strip("‪u202a"))
X1 = df.drop(['LAD','LCX','RCA','Cath'], axis = 1)
y1 = df[['RCA']]
X1_train, X1_test, y1_train, y1_test = modelselection.train_test_split(X1, y1, test_size = 0.3, random_state = 42)
imputer = SimpleImputer(missing_values = np.nan, strategy = 'median')    
imputer.fit_transform(X1_train)
X1_test = imputer.transform(X1_test)
scaler = MinMaxScaler(feature_range=(-1,1))
X1_train = scaler.fit_transform(X1_train)
X1_test = scaler.transform(X1_test)
y1_train = scaler.fit_transform(y1_train)
y1_test = scaler.transform(y1_test)
y1_train = y1_train.ravel()
y1_test = y1_test.ravel()

clf = GradientBoostingClassifier(learning_rate=1.0, max_depth=10, random_state=42)
clf.fit(X1_train, y1_train)
y1_pred = clf.predict(X1_test)

print(f'Accuracy is: {metrics.accuracy_score(y1_test,y1_pred)}\nF1 score is: {metrics.f1_score(y1_test, y1_pred)}')
sns.set(font_scale=2)
ax = sns.heatmap(metrics.confusion_matrix(y1_test,y1_pred), annot=True)  
plt.show()

